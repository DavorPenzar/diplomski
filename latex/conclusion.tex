\addchap{Zaključak}

Od razvijenih modela, za male vrijednosti najmanje svojstvene vrijednosti Laplaceovog operatora (ispod $ \numprint{200} $) najbolju alternativu klasičnom numeričkom računu predstavljaju polinomni modeli kao najbrži i najtočniji. Štoviše, za vrijednosti ispod $ \numprint{100} $ njihova točnost je $ {\pm \numprint{0.1}} $. Međutim, na višim vrijednostima njihova uspješnost znantno opada i postaju vrlo nepouzdani predviditelji. S druge strane, model neuronske mreže najtočniji je na većim vrijednostima, iako i njegova točnost na većim vrijednostima također počinje opadati. Model konvolucijske neuronske mreže među svim je modelima najlošiji na manjim vrijednostima, ali njegova točnost relativno sporije opada s porastom ciljne varijable.

\par

Razlog zašto su svi modeli znatno lošiji na većim vrijednostima mogao bi biti rijetkost zastupljenosti primjera u tim kvantilima---najviših $ \unit[\numprint{5}]{\%} $ vrijednosti u intervalu je širine više od $ \numprint{4000} $, a preostalih $ \unit[\numprint{95}]{\%} $ u intervalu je širine manje od $ \numprint{2000} $. Osim manjkavosti metodologije, moguće je i da je ciljna varijabla u tom području \emph{jako} različita za trokute koji se \emph{malo} razlikuju---ovo, uostalom, sugerira slika~\ref{fig:triangles_eigenvalues}---čime je njezinu vrijednost teže predvidjeti ako je ona jako visoka. Ipak, prije kritiziranja modela zbog nepouzdanosti na tako velikom intervalu treba uzeti u obzir sliku~\ref{fig:triangles_eigenvalues}, na kojoj se vidi da trokutima na kojima najmanja svojstvena vrijednost upada u gornjih $ \unit[\numprint{10}]{\%} $ proučavanih vrijednosti (donja granica tog kvantila iznosi otprilike $ \numprint{861.51} $, dok je minimum oko $ \numprint{52.72} $ i maksimum oko $ \numprint{5938.19} $)---\emph{najžutije}/\emph{najnarančastije} dvije zone na slici~\ref{fig:triangles_eigenvalues_contour}---vrijednost ordinate karakteristične točke iz propozicije~\ref{prop:triangle_characteristic_bijective} ne postiže ni $ \numprint{0.15} $. Preostalih $ \unit[\numprint{90}]{\%} $ trokuta, na kojima su modeli znatno uspješniji, zauzima puno veći dio skupa $ D_{{\bigtriangleup}} $ iz propozicije~\ref{prop:triangle_characteristic_bijective}.

\par

Valja uočiti da su tri kategorije modela, osim što se kategorički razlikuju, razvijene iz tri različita aspekta: linearne regresije u obzir uzimaju samo koordinate karakteristične točke iz propozicije~\ref{prop:triangle_characteristic_bijective}, neuronska mreža u obzir uzima duljine stranica, veličine vanjskih kutova i singularne vrijednosti duljina stranica i vanjskih kutova, a konvolucijska neuronska mreža u obzir uzima samo vizualizaciju trokuta. Kombinacije modela, to jest, pristupa predviđanju također predstavljaju moguće poboljšanje njihove uspješnosti. Rezultati razvijenih modela, kao i eksploratorna analiza numeričkih podataka, sugeriraju da bi takve kombinacije mogle biti uspješne.

\par%
\clearpage%
\newpage

Nastavak istraživanja predstavlja i generalizacija modela na ostale poligone. Najveći potencijal u tome ima konvolucijska neuronska mreža, a najmanji polinomna rješenja. Naime, kompleksnost konvolucijske neuronske mreže ne ovisi o broju kutova odnosno stranica tako da je na slici iste veličine moguće prikazati bilo koji oblik (i to ne nužno samo poligonalni). Doduše, pregruba rezolucija prikaza mogla bi \emph{skrivati} neke eventualno ključne informacije, ali funkcija dubine iz definicije~\ref{def:set_deepness} donekle osigurava da se oblik vizualiziranog skupa može rekonstruirati i u slučaju pregrube rezolucije. S druge strane, broj članova polinoma stupnja $ d \in \naturals $ u $ \numprint{2} k $ varijabli (koliko bi bilo koordinata karakteristične točke $ \left( k + \numprint{2} \right) $-gona), za $ k \in \naturals $, $ k \geq \numprint{3} $, iznosi $ \binom{\numprint{2} k + d}{d} $, što znači da kompleksnost polinomnih rješenja raste brzinom reda $ \bigO \left( n^{d} \right) $ s porastom broja vrhova. Iako je neuronska mreža možda \emph{prekompleksna} s obzirom na veličinu ulaza i korištene aktivacijske funkcije, jednostavnije arhitekture lučile su bitno lošije rezultate, a ovakva neuronska mreža možda bi, samo uz podešavanje veličine ulaza ili još uz eventualno malu izmjenu skrivenih slojeva, mogla predstavljati dobar model i za druge poligone. Ipak, sam ulaz s porastom broja kutova raste brzinom reda $ \bigO \left( n \right) $, stoga ni takav model nije dobar kandidat za poligone s velikim brojem vrhova.

\par

Osim rasta kompleksnosti modela, polinomni modeli i neuronska mreža ograničeni su i time da mogu predviđati nužno na jednoj klasi poligona---svi poligoni moraju imati isti broj vrhova. Da bismo konstruirali model koji bi mogao predviđati ciljnu varijablu na svim $ k $-gonima za $ k \leq m $, gdje je $ m \in \naturals $, $ m \geq \numprint{3} $, unaprijed zadan, za te modele bilo bi potrebno definirati konvenciju kojom se, na primjer, trokut prikazuje kao peterokut---to bi moglo biti tako da najdulju stranicu \emph{raspolovimo} (kao novi vrh postavljamo njezino polovište) i tako činimo dok broj vrhova nije zadovoljen---ili bi se broj primjera kod treniranja morao znatno povećati tako da se u obzir uzme što više mogućih izbora nepravih vrhova. Potonja opcija povećava kompleksnost treniranja, ali se zbog neprekidnosti spektra (\seetxt~teorem~\ref{thm:Laplace_eigenvalue_continuity}) čini boljom opcijom jer, na primjer, niz pravih osmerokuta može konvergirati k pravom sedmerokutu čak i ako nijedan vrh osmerokuta ne konvergira k polovištu neke stranice sedmerokuta.

\par

Konvolucijska neuronska mreže možda bi mogla biti uspješnija kada bi bili dostupni veći resursi, čime bi se mogla i \emph{vratiti} izostavljena $ \numprint{3} $ sloja ($ \numprint{1} $ redukcijski i $ \numprint{2} $ neredukcijska) i čime bi se mreža mogla dulje trenirati. S obzirom na mali broj epoha, a s druge strane kompleksnost ulaza i veličinu njezine arhitekture, postignuti rezultati iznenađujuće su dobri. Ionako je takva konvolucijska neuronska mreža na drugom problemu vezanom uz diferencijalne jednadžbe postizala odlične rezultate, kako su objasnili Mills i dr.\ u~\cite{bib:Mills17}, gdje su je predstavili, tako da bi valjalo daljnje istraživanje usmjeriti i prema njezinom poboljšanju.

\par%
\clearpage%
\newpage

Poboljšanja modela konvolucijske neuronske mreže mogu biti i izmjene vizualizacije---možda je izbor vizualizacije trokuta funkcijom dubine neadekvatan za ovaj problem, što bi moglo negativno utjecati na uspješnost ovog modela. Na primjer, problem bi kod odabrane vizualizacije mogao biti što rub poligona (trokuta) nije jasno vidljiv, nego vrijednosti postepeno padaju u $ \numprint{0} $ (\seetxt~sliku~\ref{fig:triangle_deepness}). Ovo bi se moglo riješiti polovičnim normiranjem funkcije dubine tako da na jednakostraničnom trokutu ona poprima $ \frac{\numprint{1}}{\numprint{2}} $ i da se takva vizualizacijska matrica tada zbroji s $ \frac{\numprint{1}}{\numprint{2}} $ vizualizacijske matrice sa slike~\ref{fig:triangle_mesh}.

\par

Međutim, s obzirom na brzine polinomnih modela i neuronske mreže, ali i njihovih točnosti barem na nekim trokutima, isplativo bi bilo baviti se i njihovim daljnjim razvoje. Na poligonima s manjim brojem vrhova, ako je ove modele moguće generalizirati i eventualno poboljšati, takvi modeli predstavljaju brzu alternativu klasičnom numeričkom računu, a pogotovo egzaktnom računu. Možda je i računanje najmanje svojstvene vrijednosti Laplaceovog operatora metodom konačnih elemenata na promatranim trokutima usporeno načinom na koji je taj program napisan i/ili kvalitetama odabranog programskog jezika \href{https://freefem.org/}{\emph{FreeFem++}}, ali tablica~\ref{tab:computation_times} sugerira da se konkurentno brzi rezultati tom metodom vjerojatno ne mogu postići.

\par

Osim teme ovog rada, s obzirom na dobivene rezultate na ovom području potencijalnim nastavkom istraživanja čine se i definirane singularne vrijednosti duljina stranica i vanjskih kutova poligona. Autor ovog rada tako definirane ni tako nazvane vrijednosti nije pronašao u literaturi, ali kroz eksploratornu analizu i dobivene rezultate, barem na trokutima, čini se da su one vrlo informativne o obliku poligona. Naime, u poglavlju~\ref{chp:Dirichlet_Laplacian} već je navedeno kako su Reuter i dr.\ objasnili u~\cite{bib:Reuter09} da je u praksi dovoljno poznavati najmanjih konačno mnogo vrijednosti spektra skupa da bi se moglo zaključivati o njegovu obliku, stoga je najmanja svojstvena vrijednost Laplaceovog operatora uvjetovana oblikom poligona. S druge strane, slika~\ref{fig:singular_value_eigenvalue}, izračunati koeficijenti korelacije i rezultati modela neuronske mreže sugeriraju da su i definirane singularne vrijednosti relativno ovisne o toj vrijednosti odnosno da postoji ovisnost u obratnom smjeru.

\par
